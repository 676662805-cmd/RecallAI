import time
import json
import os
import sys
import requests
from dotenv import load_dotenv

# å…¨å±€ state å¼•ç”¨ï¼ˆé¿å…å¾ªç¯å¯¼å…¥ï¼‰
_global_state = None

def set_global_state(state):
    """ä» main.py è®¾ç½®å…¨å±€ state å¼•ç”¨"""
    global _global_state
    _global_state = state

def get_base_path():
    """è·å–ç¨‹åºè¿è¡Œçš„åŸºç¡€è·¯å¾„ï¼Œæ”¯æŒå¼€å‘å’Œæ‰“åŒ…ç¯å¢ƒ"""
    if getattr(sys, 'frozen', False):
        # æ‰“åŒ…åçš„ exe è¿è¡Œæ—¶
        return os.path.dirname(sys.executable)
    else:
        # å¼€å‘ç¯å¢ƒ
        return os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# åŠ è½½ .env æ–‡ä»¶ï¼ˆæ”¯æŒæ‰“åŒ…åçš„è·¯å¾„ï¼‰
env_path = os.path.join(get_base_path(), '.env')
if os.path.exists(env_path):
    load_dotenv(env_path)
    print(f"[OK] Loaded .env from: {env_path}")
else:
    load_dotenv()  # å°è¯•ä»é»˜è®¤ä½ç½®åŠ è½½
    print(f"[WARN] .env not found at {env_path}, using default")

# è·å– Render äº‘ç«¯ URL
RENDER_URL = os.getenv("RENDER_URL", "https://recallai-d9sc.onrender.com")

class MatchService:
    def __init__(self):
        self.cards = self._load_cards()
        # ---  äº‘ç«¯åŒ–ï¼šç”¨æˆ· Token ---
        self.user_token = None
    
    def set_token(self, token: str):
        """è®¾ç½®ç”¨æˆ· Tokenï¼Œç”¨äºäº‘ç«¯ API é‰´æƒ"""
        self.user_token = token

    def _load_cards(self):
        try:
            base_path = get_base_path()
            file_path = os.path.join(base_path, "data", "cards.json")
            print(f"[FILE] Loading cards from: {file_path}")
            with open(file_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f"Error loading cards: {e}")
            return []
    
    def load_cards(self):
        """é‡æ–°åŠ è½½ cardsï¼ˆç”¨äºå‰ç«¯åŒæ­¥ååˆ·æ–°ï¼‰"""
        self.cards = self._load_cards()
        print(f"[RELOAD] Reloaded {len(self.cards)} cards from file")

    def find_best_match(self, user_query: str):
        
        # 1. Prepare simplified list with index-based IDs
        card_summaries = []
        for idx, c in enumerate(self.cards):
            card_summaries.append(
                f"[{idx}] Topic: {c['topic']} | Preview: {c['content'][:80]}..."
            )
        cards_text = "\n".join(card_summaries)

        # 2. Construct the Prompt (Aggressive Matching)
        system_prompt = f"""
        You are a real-time assistant for an interviewee.
        Here is the knowledge base (cards):
        {cards_text}

        Your Task:
        Predict the most likely card based on the available text, EVEN IF the sentence is incomplete.

        RULES (Aggressive Matching):
        
        1. **Keyword Priority**: 
           - If the text contains strong unique keywords matching a card topic or content, **MATCH IMMEDIATELY**. 
           - Example: "delegation" -> match [0], "async" or "promise" -> match [1], "sharding" or "database" -> match [2]
           - Do not wait for a full sentence structure.
        
        2. **Partial Context**:
           - Input: "Tell me about..." -> Look for keywords in the rest.
           - Input: "Explain delegation" -> Match [0] immediately.

        3. **Intent Filter**:
           - Try to ignore the candidate's own answers. 
           - But if ambiguous, err on the side of showing the card.

        Output JSON format (use the number in brackets):
        {{
            "best_match_index": 0
        }}
        OR if no match:
        {{
            "best_match_index": null
        }}
        """

        # --- 3. è¡¥å…¨ï¼šè°ƒç”¨äº‘ç«¯ API å‘é€è¯·æ±‚ ---
        if not self.user_token:
            print("[ERROR] No user token set! Cannot call cloud API")
            return None
        
        try:
            # å‡†å¤‡è¯·æ±‚æ•°æ®
            payload = {
                "model": "llama-3.1-8b-instant",
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"User Input: {user_query}"}
                ],
                "response_format": {"type": "json_object"},
                "temperature": 0.0
            }
            
            headers = {'Authorization': f'Bearer {self.user_token}'}
            
            # å‘é€è¯·æ±‚åˆ° Render äº‘ç«¯
            response = requests.post(
                f"{RENDER_URL}/v1/proxy/chat",
                json=payload,
                headers=headers,
                timeout=30
            )
            
            if response.status_code != 200:
                if _global_state is not None:
                    _global_state.cloud_api_error = {"status": response.status_code, "message": response.text}
                return None
            
            # 4. Parse Result
            result_data = response.json()
            result_text = result_data.get("content", "{}")
            result_json = json.loads(result_text)
            match_index = result_json.get("best_match_index")
            
            print(f"[SEARCH] AI Match Result: index={match_index}")
            print(f"[INFO] Available cards: {len(self.cards)} cards")

            # Return the full card object if found
            if match_index is not None and isinstance(match_index, int) and 0 <= match_index < len(self.cards):
                matched_card = self.cards[match_index]
                print(f"[OK] Found matching card: {matched_card['topic']}")
                return matched_card
            else:
                print(f"[WARN] No valid match (index={match_index})")
            
            return None

        except Exception as e:
            print(f"AI Match Error: {e}")
            return None

    def generate_ai_answer(self, user_query: str):
        """AI ç°åœºç”Ÿæˆé€»è¾‘"""
        print(f"ğŸ¤– AI generating for: {user_query}")
        
        system_prompt = """
        You are an Interview Coach.
        Task:
        1. **Check**: Is this input a QUESTION from an interviewer? 
           - If it is the candidate answering (e.g. "I did...", "So..."), return valid: false.
        2. **Generate**: If valid, generate a short STAR method answer.
        
        Output JSON:
        { "valid": true, "topic": "...", "content": "..." }
        OR
        { "valid": false }
        """

        if not self.user_token:
            print("[ERROR] No user token set! Cannot call cloud API")
            return None

        try:
            # å‡†å¤‡è¯·æ±‚æ•°æ®
            payload = {
                "model": "llama-3.1-8b-instant",
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_query}
                ],
                "response_format": {"type": "json_object"},
                "temperature": 0.6
            }
            
            headers = {'Authorization': f'Bearer {self.user_token}'}
            
            # å‘é€è¯·æ±‚åˆ° Render äº‘ç«¯
            response = requests.post(
                f"{RENDER_URL}/v1/proxy/chat",
                json=payload,
                headers=headers,
                timeout=30
            )
            
            if response.status_code != 200:
                if _global_state is not None:
                    _global_state.cloud_api_error = {"status": response.status_code, "message": response.text}
                return None
            
            result_data = response.json()
            result_text = result_data.get("content", "{}")
            result = json.loads(result_text)
            
            if result.get("valid"):
                return {
                    "id": f"ai_generated_{int(time.time())}", 
                    "topic": f"[NEW] AI: {result.get('topic')}", 
                    "content": result.get("content")
                }
            else:
                return None # æ ‡è®°ä¸ºæ— æ•ˆé—®é¢˜
        except Exception as e:
            print(f"Gen Error: {e}")
            return None